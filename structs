from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import *
from pyspark.sql import Row

row1 = Row(data1="2020-09-22", data2="2019-09-22", data3="2020-08-22", data4="2020-09-21")
row2 = Row(data1="2020-08-22", data2="2021-08-22", data3="2022-07-22", data4="2022-09-03")

df = spark.createDataFrame([row1, row2])

df.show()

#função que retorna struct com maior valor
def row_max_with_name(*cols):
    cols_ = [struct(col(c).alias("value"), lit(c).alias("coluna")) for c in cols]
    return greatest(*cols_).alias("greatest({0})".format(",".join(cols)))

colunas = ("data1", "data2", "data3", "data4")

df4 = df.withColumn("greatest", row_max_with_name(*colunas).coluna)


row = Row(data="2020-09-22")
row2 = Row(data="2020SEP22")



df = spark.createDataFrame([row2])
df.show()

df2 = df.withColumn("new_data", from_unixtime(unix_timestamp("data", "yyyyMMMdd"), "yyyy-MM-dd"))
df2.show()



df5 = (
    df.withColumn("now", to_timestamp(lit("2020-09-28 17:34:00"))).select("now")
        .withColumn("ontem", to_timestamp(lit("2020-09-28 01:47:00")))
        .withColumn("diff", datediff("now", "ontem"))
)

df5.show(truncate = False)


df6 = (df5
    .withColumn("now", col("now").cast(LongType()))
    .withColumn("ontem", col("ontem").cast(LongType()))
    .withColumn("diff_secs", col("now") - col("ontem"))
    .withColumn("diff_minutes", round((col("diff_secs") / 60), 2))
    .withColumn("diff_hours", round((col("diff_minutes") / 60), 2))

    )

df6.show(truncate = False)